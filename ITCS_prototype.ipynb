{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = False\n",
    "if torch.cuda.is_available():\n",
    "    is_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "\n",
    "    def __init__(self, CAPACITY):\n",
    "        self.capacity = CAPACITY \n",
    "        self.memory = []\n",
    "        self.index = 0  \n",
    "        self.save = 0\n",
    "    \n",
    "    def clear(self):\n",
    "        self.memory = []\n",
    "        self.index = 0\n",
    "        \n",
    "    def save(self,succ, ep_num,trial):\n",
    "        np.savetxt(str(succ)+\"_ep_\"+str(ep_num)+\"_\"+str(trial)+\".csv\", np.array(self.memory),fmt = \"%f\", delimiter=\",\")\n",
    "    \n",
    "    def load(self,path):\n",
    "        self.memory = np.loadtxt(\"path\", delimiter=\",\").tolist()\n",
    "\n",
    "    def stock_before(self, state, action):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append([])  # 메모리가 가득차지 않은 경우\n",
    "        else:\n",
    "            np.savetxt(\"log_\"+str(self.save)+\".csv\", np.array(self.memory),fmt = \"%f\", delimiter=\",\")\n",
    "            self.save += 1\n",
    "            self.clear()\n",
    "\n",
    "        self.memory[self.index].append(state)\n",
    "        self.memory[self.index].append(action)\n",
    "    \n",
    "    def stock_after(self,state_next,reward):\n",
    "        self.memory[self.index].append(state_next)\n",
    "        self.memory[self.index].append(reward)\n",
    "        \n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "        #print(self.memory)\n",
    "        #print(\"\")\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, n_in, n_mid, n_out):\n",
    "        \"\"\"\n",
    "        input of the layer\n",
    "        [\n",
    "             number of cars in this section, lane change stats(digitized),\n",
    "             x coordinates of cars in this section(digitized), y coordinates of cars in this section(digitized),\n",
    "             Vx of cars in this section(digitized), Vy coordinates of cars in this section(digitized),\n",
    "             ax coordinates of cars in this section(digitized), ay coordinates of cars in this section(digitized),\n",
    "             [map of the unit seciton],\n",
    "             section 0 ~ 7\n",
    "        ]\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_in, n_mid)\n",
    "        self.fc2 = nn.Linear(n_mid, n_mid)\n",
    "        self.fc3 = nn.Linear(n_mid, n_mid)\n",
    "        self.fc4 = nn.Linear(n_mid, n_mid)\n",
    "        self.fc5 = nn.Linear(n_mid, n_out)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = torch.atan(self.fc5(x))\n",
    "        output = torch.tanh(x)\n",
    "        return (output + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self,n_in,n_mid,n_out,gamma):\n",
    "        self.memory = Memory(10000)\n",
    "        self.model = Net(n_in, n_mid, n_out).double()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.01)\n",
    "        self.gamma = gamma\n",
    "        if is_cuda:\n",
    "            self.model = self.model.cuda()\n",
    "        \n",
    "    def get_param(self,path):\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "        if is_cuda:\n",
    "            self.model = self.model.cuda()\n",
    "        self.model.eval()\n",
    "\n",
    "    def save_param(self,path):\n",
    "        if is_cuda:\n",
    "            torch.save(self.model.cpu().state_dict(), path)\n",
    "        else:\n",
    "            torch.save(self.model.state_dict(), path)\n",
    "        \n",
    "    def modify_weight(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        mini_batch = self.memory.sample(BATCH_SIZE)\n",
    "        #print(mini_batch)\n",
    "        state_batch = torch.DoubleTensor(np.array(mini_batch)[:,0].tolist())\n",
    "        action_batch = torch.DoubleTensor(np.array(mini_batch)[:,1].tolist())\n",
    "        next_state_batch = torch.DoubleTensor(np.array(mini_batch)[:,2].tolist())\n",
    "        reward_batch = torch.DoubleTensor(np.array(mini_batch)[:,3].tolist())\n",
    "        \n",
    "        if is_cuda:\n",
    "            state_batch = state_batch.cuda()\n",
    "            next_state_batch = next_state_batch.cuda()\n",
    "            reward_batch = reward_batch.cuda()\n",
    "        \n",
    "        self.model.eval()\n",
    "        state_action_batch = self.model(state_batch)\n",
    "        next_state_values = self.model(next_state_batch)\n",
    "        expected_action = reward_batch + self.gamma * next_state_values\n",
    "        \"\"\"print(\"before\")\n",
    "        print(self.model.state_dict())\"\"\"\n",
    "        self.model.train()\n",
    "        \"\"\"print(\"state_action_batch\")\n",
    "        print(state_action_batch)\n",
    "        print(\"expected_action\")\n",
    "        print(expected_action)\"\"\"\n",
    "        loss = F.smooth_l1_loss(state_action_batch, expected_action)\n",
    "        \n",
    "        self.optimizer.zero_grad() \n",
    "        loss.backward() \n",
    "        self.optimizer.step()  \n",
    "        \"\"\"print(\"after\")\n",
    "        print(self.model.state_dict())\"\"\"\n",
    "        \n",
    "    def decide_action(self,state,epsilon):\n",
    "        if epsilon <= np.random.uniform(0, 1):\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                action = self.model(state)\n",
    "            if is_cuda:\n",
    "                action = action.cpu()\n",
    "        else:\n",
    "            action = torch.DoubleTensor([random.uniform(0,1), random.uniform(0,1)])\n",
    "\n",
    "        return action\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.brain = Brain(19,128,2,0.99)\n",
    "\n",
    "    def update_q_function(self):\n",
    "        self.brain.modify_weight()\n",
    "\n",
    "    def get_action(self,state,epsilon):\n",
    "        action = self.brain.decide_action(state,epsilon)\n",
    "        return action\n",
    "        \n",
    "    def save(self,succ,ep_num,trial):\n",
    "        self.brain.memory.save(succ,ep_num,trial)\n",
    "        \n",
    "    def memory_clear(self):\n",
    "        self.brain.memory.clear()\n",
    "\n",
    "    def memorize_before(self,state,action):\n",
    "        self.brain.memory.stock_before(state, action)\n",
    "    \n",
    "    def memorize_after(self,state_next,reward):\n",
    "        self.brain.memory.stock_after(state_next,reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "\n",
    "    def __init__(self,map_section, adj, car_limits, episodes, step):\n",
    "        self.map_vector = map_section\n",
    "        self.map = np.loadtxt(\"map.csv\", dtype=int, delimiter=\",\").tolist()\n",
    "        #self.map_shape = np.asarray(self.map.shape)\n",
    "        self.scalar_limit = car_limits[0]\n",
    "        self.vector_limit = car_limits[1]\n",
    "        self.noc = 0\n",
    "        self.adj = adj\n",
    "        self.episodes = episodes\n",
    "        self.step = step\n",
    "        self.dt = 0.25\n",
    "        self.agent = Agent()\n",
    "        \"\"\"\n",
    "        episodes\n",
    "        [\n",
    "            [car_scalars,car_vectors],\n",
    "            [car_scalars,car_vectors]\n",
    "        ]\n",
    "        step\n",
    "        [\n",
    "            [max trial of episodes[0],trial of episode[0]],\n",
    "            [max trial of episodes[1],trial of episode[1]]\n",
    "        ]\n",
    "        \"\"\"\n",
    "    def mk_map(self):\n",
    "        self.map  = np.full((500,500),5)\n",
    "        for i in self.map_vector[0]:\n",
    "            self.map[i[0]:i[2], i[1]] = 0\n",
    "        for i in self.map_vector[1]:\n",
    "            self.map[i[0], i[1]:i[3]] = 1\n",
    "        for i in self.map_vector[2]:\n",
    "            self.map[i[0], i[1]:i[3]] = 2\n",
    "        for i in self.map_vector[3]:\n",
    "            self.map[i[0]:i[2], i[1]] = 3\n",
    "        for i in self.map_vector[4]:\n",
    "            if i[1] == i[3]:\n",
    "                self.map[i[0]:i[2], i[1]] = 4\n",
    "            else:\n",
    "                self.map[i[0], i[1]:i[3]] = 4\n",
    "        for i in self.map_vector[6]:\n",
    "            self.map[i[0]:i[2], i[1]:i[3]] = 4\n",
    "        for i in self.map_vector[5]:\n",
    "            self.map[i[0]:i[2], i[1]:i[3]] = 5\n",
    "        np.savetxt(\"map.csv\", self.map, fmt='%i', delimiter=\",\")\n",
    "        \n",
    "    def digitize_scalar(self,car,car_limits):\n",
    "        car_temp = np.array(car).astype(int)\n",
    "        result = np.ones(len(car_limits))\n",
    "        for i in range(len(car_limits)):\n",
    "            temp = car_temp[:,i]\n",
    "            max_limit = car_limits[i]\n",
    "            result[i] = sum([(x) * ((max_limit)**y) for y, x in enumerate(temp)])\n",
    "        return result\n",
    "    def digitize_vector(self,car,car_limits):\n",
    "        car_temp = np.array(car).astype(int)\n",
    "        result = np.ones(len(car_limits))\n",
    "        for i in range(len(car_limits)):\n",
    "            temp = car_temp[:,i]\n",
    "            max_limit = car_limits[i]\n",
    "            result[i] = sum([(x+car_limits[i]) * ((car_limits[i]*2)**y) for y, x in enumerate(temp)])\n",
    "        return result\n",
    "    def decompose(self,digitized,limit,num):\n",
    "        temp = np.ones(num)\n",
    "        for i in range(num):\n",
    "            temp[i] = digitized % limit\n",
    "            digitized //= limit\n",
    "        return temp\n",
    "    def decompose_scalar(self,digi,limit,noc):\n",
    "        res = []\n",
    "        for i in range(len(digi)):\n",
    "            temp = self.decompose(digi[i],limit[i],noc)\n",
    "            res.append(temp)\n",
    "        return np.array(res).transpose()\n",
    "    def decompose_vector(self,digi,limit,noc):\n",
    "        res = []\n",
    "        digi.astype(int)\n",
    "        for i in range(len(digi)):\n",
    "            temp = self.decompose(digi[i],limit[i]*2,noc) - limit[0]\n",
    "            res.append(temp)\n",
    "        return np.array(res).transpose()\n",
    "\n",
    "    def get_state(self,time):\n",
    "        ds = self.digitize_scalar(self.car_scalar,self.scalar_limit)\n",
    "        dv = self.digitize_vector(self.car_vector,self.vector_limit)\n",
    "        concas = np.concatenate((np.array([self.noc]), ds,dv,np.array([time]),np.array(self.adj)), axis=None)\n",
    "        return torch.DoubleTensor(concas.tolist())\n",
    "    \n",
    "    def state_next(self,action):\n",
    "        #print(np.array(action)*((self.vector_limit[2]*2)**(self.noc)))\n",
    "        action = np.array(action)*((self.vector_limit[2]*2)**(self.noc))\n",
    "        res = self.decompose_vector(action,self.vector_limit[2:],self.noc)\n",
    "        #print(res)\n",
    "        for i in range(self.noc):\n",
    "            self.car_vector[i][2] += res[i][0]\n",
    "            self.car_vector[i][3] += res[i][1]\n",
    "            v_temp = [self.car_vector[i][0], self.car_vector[i][1]]\n",
    "            self.car_vector[i][0] += self.car_vector[i][2] * self.dt\n",
    "            self.car_vector[i][1] += self.car_vector[i][3] * self.dt\n",
    "            self.car_scalar[i][1] += (v_temp[0] + self.car_vector[i][0]) /2 *self.dt\n",
    "            self.car_scalar[i][2] += (v_temp[1] + self.car_vector[i][1]) /2 *self.dt\n",
    "    \n",
    "    def check(self,time,ep_num):\n",
    "        if self.noc == 0:\n",
    "            return True, [1]\n",
    "        if time == self.step[ep_num][0] - 1:\n",
    "            return False, [-1]\n",
    "        loc = np.array(self.car_scalar)[:,3:].tolist()\n",
    "        for i in range(len(loc)-1):\n",
    "            for j in range(i+1,len(loc)):\n",
    "                if (loc[i][0] == loc[j][0]) and (loc[i][1] == loc[j][1]):\n",
    "                    #print(\"collision\")\n",
    "                    return True,[-1]\n",
    "        #print(self.noc)\n",
    "        for i in range(self.noc):\n",
    "            temp = self.map[int(self.car_scalar[i][1])][int(self.car_scalar[i][2])]\n",
    "            if temp == 5:\n",
    "                #print(\"not on road\")\n",
    "                return True,[-1]\n",
    "            if self.car_vector[i][0] > self.vector_limit[0] or self.car_vector[i][1] > self.vector_limit[1]:\n",
    "                #print(\"over velocity\")\n",
    "                return False,[-1]\n",
    "            if temp == 0:\n",
    "                if self.car_vector[i][1] > 0:\n",
    "                    #print(\"reverse\")\n",
    "                    return False,[-1]\n",
    "            elif temp == 1:\n",
    "                if self.car_vector[i][0] < 0:\n",
    "                    #print(\"reverse\")\n",
    "                    return False,[-1]\n",
    "            elif temp == 2:\n",
    "                if self.car_vector[i][0] > 0:\n",
    "                    #print(\"reverse\")\n",
    "                    return False,[-1]\n",
    "            elif temp == 3:\n",
    "                if self.car_vector[i][1] < 0:\n",
    "                    #print(\"reverse\")\n",
    "                    return False,[-1]\n",
    "            if temp == 4:\n",
    "                self.car_scalar[i][0] += self.dt\n",
    "            else:\n",
    "                self.car_scalar[i][0] = 0\n",
    "        lane_stat = 0\n",
    "        des_list = []\n",
    "        for i in range(self.noc):\n",
    "            if (loc[i][0] == self.car_scalar[i][3]) and (loc[i][1] == self.car_scalar[i][4]) and self.car_vector[i][0] ==0 and self.car_vector[i][1] == 0:\n",
    "                des_list.append(i)\n",
    "                \n",
    "            if self.car_scalar[i][0] > self.scalar_limit[0]:\n",
    "                lane_stat = 1\n",
    "        \n",
    "        for i in des_list:\n",
    "            self.car_scalar = np.delete(self.car_scalar, i, axis=0)\n",
    "            self.car_vector = np.delete(self.car_vector, i, axis=0)\n",
    "            self.noc -= 1\n",
    "        \n",
    "        if lane_stat == 1:\n",
    "            return False, [-1]\n",
    "        if self.noc == 0:\n",
    "            #print(\"success\")\n",
    "            return True, [1]\n",
    "        else:\n",
    "            return False, [0]\n",
    "        \n",
    "    def run(self):\n",
    "        for ep_num in range(len(self.episodes)):\n",
    "            step_final = False\n",
    "            episode = self.episodes[ep_num]\n",
    "            complete_trial = 0\n",
    "            for trial in range(self.step[ep_num][1]):      #trial\n",
    "                \"\"\"\n",
    "                    scalar = [[lane_stat, x, y, des-x, des-y]]\n",
    "                    vector = [[vx, vy, ax, ay]]\n",
    "                \"\"\"\n",
    "                done = False\n",
    "                self.noc = len(episode[0])\n",
    "                self.car_scalar = copy.deepcopy(episode[0])\n",
    "                self.car_vector = copy.deepcopy(episode[1])\n",
    "                \n",
    "                \n",
    "                \"\"\"print(\"-----in episode \",ep_num,\" trial \",trial,\"------\")\n",
    "                print(\"episode \",episode)\n",
    "                print(\"status : \",self.car_scalar,self.car_vector)\n",
    "                print(\"<in trial \",trial,\">\")\"\"\"\n",
    "                \n",
    "                for time in range(self.step[ep_num][0]):   #stop after max trial\n",
    "                    if time == 0:\n",
    "                        state = self.get_state(time)\n",
    "                    #print(\"state: \",state)\n",
    "                    if is_cuda:\n",
    "                        state = state.cuda()\n",
    "                    #print(\"status : \",self.car_scalar,self.car_vector)\n",
    "                    action = self.agent.get_action(state,(0.8 * (1 / (trial//50 + 1))))\n",
    "                    #print(action)\n",
    "                    self.agent.memorize_before(state.cpu().tolist(),action.cpu().tolist())\n",
    "                    #print(\"before action \",action)\n",
    "                    #print(self.vector_limit)\n",
    "                    #print(\"action : \",action)\n",
    "                    self.state_next(action)\n",
    "                    \n",
    "                    done, reward = self.check(time,ep_num)\n",
    "                    #print(\"done : \",done,\"reward : \",reward)\n",
    "                    #print(\"status after : \",self.car_scalar,self.car_vector)\n",
    "                    \n",
    "                    state = self.get_state(time)\n",
    "                    self.agent.memorize_after(state.tolist(),reward)\n",
    "                    self.agent.update_q_function()\n",
    "                \n",
    "                    if done:\n",
    "                        if(reward[0] == 1):\n",
    "                            #print(self.car_vector)\n",
    "                            print(\"ep \",ep_num,\" tiral \",trial,\" success in time \",time)\n",
    "                            #self.agent.save(\"f_\",ep_num,trial)\n",
    "                            complete_trial += 1\n",
    "                        else:\n",
    "                            print(\"ep \",ep_num,\" tiral \",trial,\" failed in time \",time)\n",
    "                            #self.agent.save(\"s_\",ep_num,trial)\n",
    "                            complete_trial = 0\n",
    "                        #self.agent.memory_clear()\n",
    "                        break\n",
    "                        \n",
    "                if complete_trial>10:\n",
    "                    break\n",
    "                    \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SoonBeom\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: loadtxt: Empty input file: \"map5.csv\"\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "map = [np.loadtxt(\"map0.csv\", dtype=int, delimiter=\",\").tolist(),\n",
    "       np.loadtxt(\"map1.csv\", dtype=int, delimiter=\",\").tolist(),\n",
    "       np.loadtxt(\"map2.csv\", dtype=int, delimiter=\",\").tolist(),\n",
    "       np.loadtxt(\"map3.csv\", dtype=int, delimiter=\",\").tolist(),\n",
    "       np.loadtxt(\"map4.csv\", dtype=int, delimiter=\",\").tolist(),\n",
    "       np.loadtxt(\"map5.csv\", dtype=int, delimiter=\",\").tolist(),\n",
    "       np.loadtxt(\"map6.csv\", dtype=int, delimiter=\",\").tolist(),\n",
    "      ]\n",
    "adj = [-1,-1,-1,-1,-1,-1,-1,-1]\n",
    "car_limit = [[4,500,500,500,500],[70,70,25,25]]\n",
    "ep = [[[[0,0,300,280,300]],[[0,0,0,0]]],[[[0,299,317,100,318]],[[0,0,0,0]]],[[[0,0,300,280,300],[0,299,317,100,318]],[[0,0,0,0],[0,0,0,0]]]]\n",
    "step = [[500,200],[240,200],[240,200]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep  0  tiral  0  failed in time  0\n",
      "ep  0  tiral  1  failed in time  5\n",
      "ep  0  tiral  2  failed in time  4\n",
      "ep  0  tiral  3  failed in time  0\n",
      "ep  0  tiral  4  failed in time  3\n",
      "ep  0  tiral  5  failed in time  0\n",
      "ep  0  tiral  6  failed in time  0\n",
      "ep  0  tiral  7  failed in time  6\n",
      "ep  0  tiral  8  failed in time  0\n",
      "ep  0  tiral  9  failed in time  0\n",
      "ep  0  tiral  10  failed in time  1\n",
      "ep  0  tiral  11  failed in time  2\n",
      "ep  0  tiral  12  failed in time  0\n",
      "ep  0  tiral  13  failed in time  0\n",
      "ep  0  tiral  14  failed in time  4\n",
      "ep  0  tiral  15  failed in time  0\n",
      "ep  0  tiral  16  failed in time  3\n",
      "ep  0  tiral  17  failed in time  3\n",
      "ep  0  tiral  18  failed in time  2\n",
      "ep  0  tiral  19  failed in time  0\n",
      "ep  0  tiral  20  failed in time  4\n",
      "ep  0  tiral  21  failed in time  0\n",
      "ep  0  tiral  22  failed in time  0\n",
      "ep  0  tiral  23  failed in time  5\n",
      "ep  0  tiral  24  failed in time  0\n",
      "ep  0  tiral  25  failed in time  2\n",
      "ep  0  tiral  26  failed in time  4\n",
      "ep  0  tiral  27  failed in time  0\n",
      "ep  0  tiral  28  failed in time  4\n",
      "ep  0  tiral  29  failed in time  0\n",
      "ep  0  tiral  30  failed in time  5\n",
      "ep  0  tiral  31  failed in time  0\n",
      "ep  0  tiral  32  failed in time  0\n",
      "ep  0  tiral  33  failed in time  3\n",
      "ep  0  tiral  34  failed in time  0\n",
      "ep  0  tiral  35  failed in time  2\n",
      "ep  0  tiral  36  failed in time  4\n",
      "ep  0  tiral  37  failed in time  0\n",
      "ep  0  tiral  38  failed in time  4\n",
      "ep  0  tiral  39  failed in time  4\n",
      "ep  0  tiral  40  failed in time  0\n",
      "ep  0  tiral  41  failed in time  0\n",
      "ep  0  tiral  42  failed in time  0\n",
      "ep  0  tiral  43  failed in time  0\n",
      "ep  0  tiral  44  failed in time  0\n",
      "ep  0  tiral  45  failed in time  0\n",
      "ep  0  tiral  46  failed in time  0\n",
      "ep  0  tiral  47  failed in time  0\n",
      "ep  0  tiral  48  failed in time  0\n",
      "ep  0  tiral  49  failed in time  0\n",
      "ep  0  tiral  50  failed in time  0\n",
      "ep  0  tiral  51  failed in time  0\n",
      "ep  0  tiral  52  failed in time  0\n",
      "ep  0  tiral  53  failed in time  0\n",
      "ep  0  tiral  54  failed in time  0\n",
      "ep  0  tiral  55  failed in time  2\n",
      "ep  0  tiral  56  failed in time  0\n",
      "ep  0  tiral  57  failed in time  4\n",
      "ep  0  tiral  58  failed in time  6\n",
      "ep  0  tiral  59  failed in time  0\n",
      "ep  0  tiral  60  failed in time  4\n",
      "ep  0  tiral  61  failed in time  0\n",
      "ep  0  tiral  62  failed in time  0\n",
      "ep  0  tiral  63  failed in time  0\n",
      "ep  0  tiral  64  failed in time  0\n",
      "ep  0  tiral  65  failed in time  3\n",
      "ep  0  tiral  66  failed in time  0\n",
      "ep  0  tiral  67  failed in time  0\n",
      "ep  0  tiral  68  failed in time  0\n",
      "ep  0  tiral  69  failed in time  0\n",
      "ep  0  tiral  70  failed in time  0\n",
      "ep  0  tiral  71  failed in time  0\n",
      "ep  0  tiral  72  failed in time  3\n",
      "ep  0  tiral  73  failed in time  0\n",
      "ep  0  tiral  74  failed in time  0\n",
      "ep  0  tiral  75  failed in time  0\n",
      "ep  0  tiral  76  failed in time  0\n",
      "ep  0  tiral  77  failed in time  1\n",
      "ep  0  tiral  78  failed in time  0\n",
      "ep  0  tiral  79  failed in time  0\n",
      "ep  0  tiral  80  failed in time  0\n",
      "ep  0  tiral  81  failed in time  0\n",
      "ep  0  tiral  82  failed in time  0\n",
      "ep  0  tiral  83  failed in time  0\n",
      "ep  0  tiral  84  failed in time  0\n",
      "ep  0  tiral  85  failed in time  0\n",
      "ep  0  tiral  86  failed in time  0\n",
      "ep  0  tiral  87  failed in time  0\n",
      "ep  0  tiral  88  failed in time  0\n",
      "ep  0  tiral  89  failed in time  0\n",
      "ep  0  tiral  90  failed in time  0\n",
      "ep  0  tiral  91  failed in time  0\n",
      "ep  0  tiral  92  failed in time  5\n",
      "ep  0  tiral  93  failed in time  0\n",
      "ep  0  tiral  94  failed in time  0\n",
      "ep  0  tiral  95  failed in time  3\n",
      "ep  0  tiral  96  failed in time  0\n",
      "ep  0  tiral  97  failed in time  0\n",
      "ep  0  tiral  98  failed in time  5\n",
      "ep  0  tiral  99  failed in time  4\n",
      "ep  0  tiral  100  failed in time  4\n",
      "ep  0  tiral  101  failed in time  0\n",
      "ep  0  tiral  102  failed in time  0\n",
      "ep  0  tiral  103  failed in time  0\n",
      "ep  0  tiral  104  failed in time  0\n",
      "ep  0  tiral  105  failed in time  0\n",
      "ep  0  tiral  106  failed in time  0\n",
      "ep  0  tiral  107  failed in time  0\n",
      "ep  0  tiral  108  failed in time  0\n",
      "ep  0  tiral  109  failed in time  0\n",
      "ep  0  tiral  110  failed in time  0\n",
      "ep  0  tiral  111  failed in time  0\n",
      "ep  0  tiral  112  failed in time  0\n",
      "ep  0  tiral  113  failed in time  0\n",
      "ep  0  tiral  114  failed in time  0\n",
      "ep  0  tiral  115  failed in time  0\n",
      "ep  0  tiral  116  failed in time  0\n",
      "ep  0  tiral  117  failed in time  0\n",
      "ep  0  tiral  118  failed in time  2\n",
      "ep  0  tiral  119  failed in time  0\n",
      "ep  0  tiral  120  failed in time  1\n",
      "ep  0  tiral  121  failed in time  0\n",
      "ep  0  tiral  122  failed in time  4\n",
      "ep  0  tiral  123  failed in time  0\n",
      "ep  0  tiral  124  failed in time  0\n",
      "ep  0  tiral  125  failed in time  1\n",
      "ep  0  tiral  126  failed in time  0\n",
      "ep  0  tiral  127  failed in time  0\n",
      "ep  0  tiral  128  failed in time  0\n",
      "ep  0  tiral  129  failed in time  0\n",
      "ep  0  tiral  130  failed in time  0\n",
      "ep  0  tiral  131  failed in time  0\n",
      "ep  0  tiral  132  failed in time  0\n",
      "ep  0  tiral  133  failed in time  0\n",
      "ep  0  tiral  134  failed in time  0\n",
      "ep  0  tiral  135  failed in time  0\n",
      "ep  0  tiral  136  failed in time  0\n",
      "ep  0  tiral  137  failed in time  0\n",
      "ep  0  tiral  138  failed in time  0\n",
      "ep  0  tiral  139  failed in time  0\n",
      "ep  0  tiral  140  failed in time  0\n",
      "ep  0  tiral  141  failed in time  0\n",
      "ep  0  tiral  142  failed in time  0\n",
      "ep  0  tiral  143  failed in time  0\n",
      "ep  0  tiral  144  failed in time  0\n",
      "ep  0  tiral  145  failed in time  0\n",
      "ep  0  tiral  146  failed in time  0\n",
      "ep  0  tiral  147  failed in time  0\n",
      "ep  0  tiral  148  failed in time  0\n",
      "ep  0  tiral  149  failed in time  0\n",
      "ep  0  tiral  150  failed in time  0\n",
      "ep  0  tiral  151  failed in time  0\n",
      "ep  0  tiral  152  failed in time  0\n",
      "ep  0  tiral  153  failed in time  0\n",
      "ep  0  tiral  154  failed in time  0\n",
      "ep  0  tiral  155  failed in time  0\n",
      "ep  0  tiral  156  failed in time  0\n",
      "ep  0  tiral  157  failed in time  0\n",
      "ep  0  tiral  158  failed in time  0\n",
      "ep  0  tiral  159  failed in time  0\n",
      "ep  0  tiral  160  failed in time  0\n",
      "ep  0  tiral  161  failed in time  0\n",
      "ep  0  tiral  162  failed in time  0\n",
      "ep  0  tiral  163  failed in time  0\n",
      "ep  0  tiral  164  failed in time  0\n",
      "ep  0  tiral  165  failed in time  1\n",
      "ep  0  tiral  166  failed in time  0\n",
      "ep  0  tiral  167  failed in time  1\n",
      "ep  0  tiral  168  failed in time  0\n",
      "ep  0  tiral  169  failed in time  0\n",
      "ep  0  tiral  170  failed in time  0\n",
      "ep  0  tiral  171  failed in time  0\n",
      "ep  0  tiral  172  failed in time  0\n",
      "ep  0  tiral  173  failed in time  0\n",
      "ep  0  tiral  174  failed in time  0\n",
      "ep  0  tiral  175  failed in time  0\n",
      "ep  0  tiral  176  failed in time  0\n",
      "ep  0  tiral  177  failed in time  3\n",
      "ep  0  tiral  178  failed in time  0\n",
      "ep  0  tiral  179  failed in time  0\n",
      "ep  0  tiral  180  failed in time  0\n",
      "ep  0  tiral  181  failed in time  0\n",
      "ep  0  tiral  182  failed in time  4\n",
      "ep  0  tiral  183  failed in time  0\n",
      "ep  0  tiral  184  failed in time  0\n",
      "ep  0  tiral  185  failed in time  0\n",
      "ep  0  tiral  186  failed in time  0\n",
      "ep  0  tiral  187  failed in time  0\n",
      "ep  0  tiral  188  failed in time  0\n",
      "ep  0  tiral  189  failed in time  0\n",
      "ep  0  tiral  190  failed in time  0\n",
      "ep  0  tiral  191  failed in time  5\n",
      "ep  0  tiral  192  failed in time  0\n",
      "ep  0  tiral  193  failed in time  0\n",
      "ep  0  tiral  194  failed in time  0\n",
      "ep  0  tiral  195  failed in time  0\n",
      "ep  0  tiral  196  failed in time  0\n",
      "ep  0  tiral  197  failed in time  0\n",
      "ep  0  tiral  198  failed in time  0\n",
      "ep  0  tiral  199  failed in time  0\n",
      "ep  1  tiral  0  failed in time  1\n",
      "ep  1  tiral  1  failed in time  6\n",
      "ep  1  tiral  2  failed in time  4\n",
      "ep  1  tiral  3  failed in time  5\n",
      "ep  1  tiral  4  failed in time  8\n",
      "ep  1  tiral  5  failed in time  3\n",
      "ep  1  tiral  6  failed in time  4\n",
      "ep  1  tiral  7  failed in time  7\n",
      "ep  1  tiral  8  failed in time  1\n",
      "ep  1  tiral  9  failed in time  3\n",
      "ep  1  tiral  10  failed in time  3\n",
      "ep  1  tiral  11  failed in time  10\n",
      "ep  1  tiral  12  failed in time  5\n",
      "ep  1  tiral  13  failed in time  2\n",
      "ep  1  tiral  14  failed in time  6\n",
      "ep  1  tiral  15  failed in time  6\n",
      "ep  1  tiral  16  failed in time  5\n",
      "ep  1  tiral  17  failed in time  4\n",
      "ep  1  tiral  18  failed in time  4\n",
      "ep  1  tiral  19  failed in time  5\n",
      "ep  1  tiral  20  failed in time  4\n",
      "ep  1  tiral  21  failed in time  2\n",
      "ep  1  tiral  22  failed in time  3\n",
      "ep  1  tiral  23  failed in time  3\n",
      "ep  1  tiral  24  failed in time  12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep  1  tiral  25  failed in time  8\n",
      "ep  1  tiral  26  failed in time  5\n",
      "ep  1  tiral  27  failed in time  1\n",
      "ep  1  tiral  28  failed in time  2\n",
      "ep  1  tiral  29  failed in time  1\n",
      "ep  1  tiral  30  failed in time  4\n",
      "ep  1  tiral  31  failed in time  7\n",
      "ep  1  tiral  32  failed in time  8\n",
      "ep  1  tiral  33  failed in time  3\n",
      "ep  1  tiral  34  failed in time  3\n",
      "ep  1  tiral  35  failed in time  4\n",
      "ep  1  tiral  36  failed in time  3\n",
      "ep  1  tiral  37  failed in time  9\n",
      "ep  1  tiral  38  failed in time  5\n",
      "ep  1  tiral  39  failed in time  8\n",
      "ep  1  tiral  40  failed in time  5\n",
      "ep  1  tiral  41  failed in time  2\n",
      "ep  1  tiral  42  failed in time  8\n",
      "ep  1  tiral  43  failed in time  2\n",
      "ep  1  tiral  44  failed in time  4\n",
      "ep  1  tiral  45  failed in time  4\n",
      "ep  1  tiral  46  failed in time  1\n",
      "ep  1  tiral  47  failed in time  6\n",
      "ep  1  tiral  48  failed in time  1\n",
      "ep  1  tiral  49  failed in time  8\n",
      "ep  1  tiral  50  failed in time  3\n",
      "ep  1  tiral  51  failed in time  4\n",
      "ep  1  tiral  52  failed in time  4\n",
      "ep  1  tiral  53  failed in time  4\n",
      "ep  1  tiral  54  failed in time  6\n",
      "ep  1  tiral  55  failed in time  3\n",
      "ep  1  tiral  56  failed in time  4\n",
      "ep  1  tiral  57  failed in time  3\n",
      "ep  1  tiral  58  failed in time  3\n",
      "ep  1  tiral  59  failed in time  2\n",
      "ep  1  tiral  60  failed in time  5\n",
      "ep  1  tiral  61  failed in time  8\n",
      "ep  1  tiral  62  failed in time  5\n",
      "ep  1  tiral  63  failed in time  7\n",
      "ep  1  tiral  64  failed in time  3\n",
      "ep  1  tiral  65  failed in time  3\n",
      "ep  1  tiral  66  failed in time  3\n",
      "ep  1  tiral  67  failed in time  7\n",
      "ep  1  tiral  68  failed in time  5\n",
      "ep  1  tiral  69  failed in time  3\n",
      "ep  1  tiral  70  failed in time  4\n",
      "ep  1  tiral  71  failed in time  5\n",
      "ep  1  tiral  72  failed in time  5\n",
      "ep  1  tiral  73  failed in time  4\n",
      "ep  1  tiral  74  failed in time  3\n",
      "ep  1  tiral  75  failed in time  5\n",
      "ep  1  tiral  76  failed in time  3\n",
      "ep  1  tiral  77  failed in time  4\n",
      "ep  1  tiral  78  failed in time  3\n",
      "ep  1  tiral  79  failed in time  5\n",
      "ep  1  tiral  80  failed in time  3\n",
      "ep  1  tiral  81  failed in time  3\n",
      "ep  1  tiral  82  failed in time  3\n",
      "ep  1  tiral  83  failed in time  6\n",
      "ep  1  tiral  84  failed in time  3\n",
      "ep  1  tiral  85  failed in time  4\n",
      "ep  1  tiral  86  failed in time  6\n",
      "ep  1  tiral  87  failed in time  3\n",
      "ep  1  tiral  88  failed in time  3\n",
      "ep  1  tiral  89  failed in time  3\n",
      "ep  1  tiral  90  failed in time  1\n",
      "ep  1  tiral  91  failed in time  3\n",
      "ep  1  tiral  92  failed in time  8\n",
      "ep  1  tiral  93  failed in time  3\n",
      "ep  1  tiral  94  failed in time  3\n",
      "ep  1  tiral  95  failed in time  4\n",
      "ep  1  tiral  96  failed in time  5\n",
      "ep  1  tiral  97  failed in time  3\n",
      "ep  1  tiral  98  failed in time  5\n",
      "ep  1  tiral  99  failed in time  2\n",
      "ep  1  tiral  100  failed in time  10\n",
      "ep  1  tiral  101  failed in time  3\n",
      "ep  1  tiral  102  failed in time  4\n",
      "ep  1  tiral  103  failed in time  3\n",
      "ep  1  tiral  104  failed in time  3\n",
      "ep  1  tiral  105  failed in time  3\n",
      "ep  1  tiral  106  failed in time  2\n",
      "ep  1  tiral  107  failed in time  3\n",
      "ep  1  tiral  108  failed in time  3\n",
      "ep  1  tiral  109  failed in time  3\n",
      "ep  1  tiral  110  failed in time  3\n",
      "ep  1  tiral  111  failed in time  3\n",
      "ep  1  tiral  112  failed in time  4\n",
      "ep  1  tiral  113  failed in time  3\n",
      "ep  1  tiral  114  failed in time  4\n",
      "ep  1  tiral  115  failed in time  3\n",
      "ep  1  tiral  116  failed in time  3\n",
      "ep  1  tiral  117  failed in time  5\n",
      "ep  1  tiral  118  failed in time  3\n",
      "ep  1  tiral  119  failed in time  3\n",
      "ep  1  tiral  120  failed in time  3\n",
      "ep  1  tiral  121  failed in time  3\n",
      "ep  1  tiral  122  failed in time  1\n",
      "ep  1  tiral  123  failed in time  3\n",
      "ep  1  tiral  124  failed in time  3\n",
      "ep  1  tiral  125  failed in time  3\n",
      "ep  1  tiral  126  failed in time  3\n",
      "ep  1  tiral  127  failed in time  3\n",
      "ep  1  tiral  128  failed in time  6\n",
      "ep  1  tiral  129  failed in time  4\n",
      "ep  1  tiral  130  failed in time  4\n",
      "ep  1  tiral  131  failed in time  3\n",
      "ep  1  tiral  132  failed in time  3\n",
      "ep  1  tiral  133  failed in time  3\n",
      "ep  1  tiral  134  failed in time  5\n",
      "ep  1  tiral  135  failed in time  3\n",
      "ep  1  tiral  136  failed in time  4\n",
      "ep  1  tiral  137  failed in time  4\n",
      "ep  1  tiral  138  failed in time  5\n",
      "ep  1  tiral  139  failed in time  3\n",
      "ep  1  tiral  140  failed in time  5\n",
      "ep  1  tiral  141  failed in time  4\n",
      "ep  1  tiral  142  failed in time  3\n",
      "ep  1  tiral  143  failed in time  3\n",
      "ep  1  tiral  144  failed in time  5\n",
      "ep  1  tiral  145  failed in time  3\n",
      "ep  1  tiral  146  failed in time  2\n",
      "ep  1  tiral  147  failed in time  2\n",
      "ep  1  tiral  148  failed in time  5\n",
      "ep  1  tiral  149  failed in time  2\n",
      "ep  1  tiral  150  failed in time  3\n",
      "ep  1  tiral  151  failed in time  3\n",
      "ep  1  tiral  152  failed in time  3\n",
      "ep  1  tiral  153  failed in time  3\n",
      "ep  1  tiral  154  failed in time  5\n",
      "ep  1  tiral  155  failed in time  3\n",
      "ep  1  tiral  156  failed in time  3\n",
      "ep  1  tiral  157  failed in time  6\n",
      "ep  1  tiral  158  failed in time  3\n",
      "ep  1  tiral  159  failed in time  3\n",
      "ep  1  tiral  160  failed in time  2\n",
      "ep  1  tiral  161  failed in time  3\n",
      "ep  1  tiral  162  failed in time  5\n",
      "ep  1  tiral  163  failed in time  4\n",
      "ep  1  tiral  164  failed in time  3\n",
      "ep  1  tiral  165  failed in time  4\n",
      "ep  1  tiral  166  failed in time  3\n",
      "ep  1  tiral  167  failed in time  3\n",
      "ep  1  tiral  168  failed in time  3\n",
      "ep  1  tiral  169  failed in time  4\n",
      "ep  1  tiral  170  failed in time  4\n",
      "ep  1  tiral  171  failed in time  3\n",
      "ep  1  tiral  172  failed in time  3\n",
      "ep  1  tiral  173  failed in time  5\n",
      "ep  1  tiral  174  failed in time  3\n",
      "ep  1  tiral  175  failed in time  4\n",
      "ep  1  tiral  176  failed in time  3\n",
      "ep  1  tiral  177  failed in time  3\n",
      "ep  1  tiral  178  failed in time  4\n",
      "ep  1  tiral  179  failed in time  3\n",
      "ep  1  tiral  180  failed in time  3\n",
      "ep  1  tiral  181  failed in time  3\n",
      "ep  1  tiral  182  failed in time  3\n",
      "ep  1  tiral  183  failed in time  3\n",
      "ep  1  tiral  184  failed in time  2\n",
      "ep  1  tiral  185  failed in time  3\n",
      "ep  1  tiral  186  failed in time  3\n",
      "ep  1  tiral  187  failed in time  4\n",
      "ep  1  tiral  188  failed in time  5\n",
      "ep  1  tiral  189  failed in time  3\n",
      "ep  1  tiral  190  failed in time  3\n",
      "ep  1  tiral  191  failed in time  4\n",
      "ep  1  tiral  192  failed in time  3\n",
      "ep  1  tiral  193  failed in time  3\n",
      "ep  1  tiral  194  failed in time  5\n",
      "ep  1  tiral  195  failed in time  3\n",
      "ep  1  tiral  196  failed in time  4\n",
      "ep  1  tiral  197  failed in time  3\n",
      "ep  1  tiral  198  failed in time  3\n",
      "ep  1  tiral  199  failed in time  3\n",
      "ep  2  tiral  0  failed in time  0\n",
      "ep  2  tiral  1  failed in time  6\n",
      "ep  2  tiral  2  failed in time  1\n",
      "ep  2  tiral  3  failed in time  2\n",
      "ep  2  tiral  4  failed in time  0\n",
      "ep  2  tiral  5  failed in time  0\n",
      "ep  2  tiral  6  failed in time  4\n",
      "ep  2  tiral  7  failed in time  0\n",
      "ep  2  tiral  8  failed in time  0\n",
      "ep  2  tiral  9  failed in time  5\n",
      "ep  2  tiral  10  failed in time  2\n",
      "ep  2  tiral  11  failed in time  0\n",
      "ep  2  tiral  12  failed in time  0\n",
      "ep  2  tiral  13  failed in time  12\n",
      "ep  2  tiral  14  failed in time  4\n",
      "ep  2  tiral  15  failed in time  0\n",
      "ep  2  tiral  16  failed in time  2\n",
      "ep  2  tiral  17  failed in time  0\n",
      "ep  2  tiral  18  failed in time  4\n",
      "ep  2  tiral  19  failed in time  0\n",
      "ep  2  tiral  20  failed in time  0\n",
      "ep  2  tiral  21  failed in time  0\n",
      "ep  2  tiral  22  failed in time  0\n",
      "ep  2  tiral  23  failed in time  0\n",
      "ep  2  tiral  24  failed in time  0\n",
      "ep  2  tiral  25  failed in time  2\n",
      "ep  2  tiral  26  failed in time  0\n",
      "ep  2  tiral  27  failed in time  4\n",
      "ep  2  tiral  28  failed in time  3\n",
      "ep  2  tiral  29  failed in time  0\n",
      "ep  2  tiral  30  failed in time  0\n",
      "ep  2  tiral  31  failed in time  0\n",
      "ep  2  tiral  32  failed in time  5\n",
      "ep  2  tiral  33  failed in time  0\n",
      "ep  2  tiral  34  failed in time  3\n",
      "ep  2  tiral  35  failed in time  0\n",
      "ep  2  tiral  36  failed in time  0\n",
      "ep  2  tiral  37  failed in time  6\n",
      "ep  2  tiral  38  failed in time  3\n",
      "ep  2  tiral  39  failed in time  6\n",
      "ep  2  tiral  40  failed in time  0\n",
      "ep  2  tiral  41  failed in time  0\n",
      "ep  2  tiral  42  failed in time  6\n",
      "ep  2  tiral  43  failed in time  0\n",
      "ep  2  tiral  44  failed in time  0\n",
      "ep  2  tiral  45  failed in time  1\n",
      "ep  2  tiral  46  failed in time  0\n",
      "ep  2  tiral  47  failed in time  0\n",
      "ep  2  tiral  48  failed in time  3\n",
      "ep  2  tiral  49  failed in time  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep  2  tiral  50  failed in time  1\n",
      "ep  2  tiral  51  failed in time  0\n",
      "ep  2  tiral  52  failed in time  0\n",
      "ep  2  tiral  53  failed in time  0\n",
      "ep  2  tiral  54  failed in time  0\n",
      "ep  2  tiral  55  failed in time  0\n",
      "ep  2  tiral  56  failed in time  0\n",
      "ep  2  tiral  57  failed in time  0\n",
      "ep  2  tiral  58  failed in time  0\n",
      "ep  2  tiral  59  failed in time  0\n",
      "ep  2  tiral  60  failed in time  0\n",
      "ep  2  tiral  61  failed in time  0\n",
      "ep  2  tiral  62  failed in time  0\n",
      "ep  2  tiral  63  failed in time  4\n",
      "ep  2  tiral  64  failed in time  3\n",
      "ep  2  tiral  65  failed in time  4\n",
      "ep  2  tiral  66  failed in time  0\n",
      "ep  2  tiral  67  failed in time  0\n",
      "ep  2  tiral  68  failed in time  0\n",
      "ep  2  tiral  69  failed in time  0\n",
      "ep  2  tiral  70  failed in time  0\n",
      "ep  2  tiral  71  failed in time  0\n",
      "ep  2  tiral  72  failed in time  0\n",
      "ep  2  tiral  73  failed in time  8\n",
      "ep  2  tiral  74  failed in time  0\n",
      "ep  2  tiral  75  failed in time  3\n",
      "ep  2  tiral  76  failed in time  0\n",
      "ep  2  tiral  77  failed in time  0\n",
      "ep  2  tiral  78  failed in time  2\n",
      "ep  2  tiral  79  failed in time  6\n",
      "ep  2  tiral  80  failed in time  6\n",
      "ep  2  tiral  81  failed in time  0\n",
      "ep  2  tiral  82  failed in time  0\n",
      "ep  2  tiral  83  failed in time  0\n",
      "ep  2  tiral  84  failed in time  0\n",
      "ep  2  tiral  85  failed in time  0\n",
      "ep  2  tiral  86  failed in time  0\n",
      "ep  2  tiral  87  failed in time  0\n",
      "ep  2  tiral  88  failed in time  0\n",
      "ep  2  tiral  89  failed in time  5\n",
      "ep  2  tiral  90  failed in time  0\n",
      "ep  2  tiral  91  failed in time  0\n",
      "ep  2  tiral  92  failed in time  0\n",
      "ep  2  tiral  93  failed in time  0\n",
      "ep  2  tiral  94  failed in time  1\n",
      "ep  2  tiral  95  failed in time  0\n",
      "ep  2  tiral  96  failed in time  2\n",
      "ep  2  tiral  97  failed in time  0\n",
      "ep  2  tiral  98  failed in time  1\n",
      "ep  2  tiral  99  failed in time  0\n",
      "ep  2  tiral  100  failed in time  4\n",
      "ep  2  tiral  101  failed in time  0\n",
      "ep  2  tiral  102  failed in time  0\n",
      "ep  2  tiral  103  failed in time  0\n",
      "ep  2  tiral  104  failed in time  0\n",
      "ep  2  tiral  105  failed in time  0\n",
      "ep  2  tiral  106  failed in time  0\n",
      "ep  2  tiral  107  failed in time  0\n",
      "ep  2  tiral  108  failed in time  0\n",
      "ep  2  tiral  109  failed in time  0\n",
      "ep  2  tiral  110  failed in time  0\n",
      "ep  2  tiral  111  failed in time  7\n",
      "ep  2  tiral  112  failed in time  0\n",
      "ep  2  tiral  113  failed in time  1\n",
      "ep  2  tiral  114  failed in time  0\n",
      "ep  2  tiral  115  failed in time  0\n",
      "ep  2  tiral  116  failed in time  0\n",
      "ep  2  tiral  117  failed in time  0\n",
      "ep  2  tiral  118  failed in time  0\n",
      "ep  2  tiral  119  failed in time  0\n",
      "ep  2  tiral  120  failed in time  0\n",
      "ep  2  tiral  121  failed in time  0\n",
      "ep  2  tiral  122  failed in time  3\n",
      "ep  2  tiral  123  failed in time  0\n",
      "ep  2  tiral  124  failed in time  0\n",
      "ep  2  tiral  125  failed in time  0\n",
      "ep  2  tiral  126  failed in time  0\n",
      "ep  2  tiral  127  failed in time  2\n",
      "ep  2  tiral  128  failed in time  0\n",
      "ep  2  tiral  129  failed in time  3\n",
      "ep  2  tiral  130  failed in time  0\n",
      "ep  2  tiral  131  failed in time  0\n",
      "ep  2  tiral  132  failed in time  0\n",
      "ep  2  tiral  133  failed in time  0\n",
      "ep  2  tiral  134  failed in time  0\n",
      "ep  2  tiral  135  failed in time  0\n",
      "ep  2  tiral  136  failed in time  0\n",
      "ep  2  tiral  137  failed in time  0\n",
      "ep  2  tiral  138  failed in time  3\n",
      "ep  2  tiral  139  failed in time  0\n",
      "ep  2  tiral  140  failed in time  0\n",
      "ep  2  tiral  141  failed in time  0\n",
      "ep  2  tiral  142  failed in time  0\n",
      "ep  2  tiral  143  failed in time  0\n",
      "ep  2  tiral  144  failed in time  0\n",
      "ep  2  tiral  145  failed in time  0\n",
      "ep  2  tiral  146  failed in time  1\n",
      "ep  2  tiral  147  failed in time  0\n",
      "ep  2  tiral  148  failed in time  1\n",
      "ep  2  tiral  149  failed in time  0\n",
      "ep  2  tiral  150  failed in time  0\n",
      "ep  2  tiral  151  failed in time  0\n",
      "ep  2  tiral  152  failed in time  0\n",
      "ep  2  tiral  153  failed in time  0\n",
      "ep  2  tiral  154  failed in time  0\n",
      "ep  2  tiral  155  failed in time  0\n",
      "ep  2  tiral  156  failed in time  0\n",
      "ep  2  tiral  157  failed in time  0\n",
      "ep  2  tiral  158  failed in time  0\n",
      "ep  2  tiral  159  failed in time  0\n",
      "ep  2  tiral  160  failed in time  0\n",
      "ep  2  tiral  161  failed in time  0\n",
      "ep  2  tiral  162  failed in time  0\n",
      "ep  2  tiral  163  failed in time  0\n",
      "ep  2  tiral  164  failed in time  0\n",
      "ep  2  tiral  165  failed in time  0\n",
      "ep  2  tiral  166  failed in time  0\n",
      "ep  2  tiral  167  failed in time  0\n",
      "ep  2  tiral  168  failed in time  0\n",
      "ep  2  tiral  169  failed in time  4\n",
      "ep  2  tiral  170  failed in time  0\n",
      "ep  2  tiral  171  failed in time  3\n",
      "ep  2  tiral  172  failed in time  5\n",
      "ep  2  tiral  173  failed in time  0\n",
      "ep  2  tiral  174  failed in time  0\n",
      "ep  2  tiral  175  failed in time  0\n",
      "ep  2  tiral  176  failed in time  0\n",
      "ep  2  tiral  177  failed in time  0\n",
      "ep  2  tiral  178  failed in time  0\n",
      "ep  2  tiral  179  failed in time  0\n",
      "ep  2  tiral  180  failed in time  2\n",
      "ep  2  tiral  181  failed in time  1\n",
      "ep  2  tiral  182  failed in time  0\n",
      "ep  2  tiral  183  failed in time  0\n",
      "ep  2  tiral  184  failed in time  0\n",
      "ep  2  tiral  185  failed in time  0\n",
      "ep  2  tiral  186  failed in time  0\n",
      "ep  2  tiral  187  failed in time  0\n",
      "ep  2  tiral  188  failed in time  0\n",
      "ep  2  tiral  189  failed in time  0\n",
      "ep  2  tiral  190  failed in time  0\n",
      "ep  2  tiral  191  failed in time  2\n",
      "ep  2  tiral  192  failed in time  0\n",
      "ep  2  tiral  193  failed in time  0\n",
      "ep  2  tiral  194  failed in time  0\n",
      "ep  2  tiral  195  failed in time  3\n",
      "ep  2  tiral  196  failed in time  3\n",
      "ep  2  tiral  197  failed in time  0\n",
      "ep  2  tiral  198  failed in time  0\n",
      "ep  2  tiral  199  failed in time  1\n",
      "OrderedDict([('fc1.weight', tensor([[-0.2471, -0.0212,  0.5268,  ...,  0.3369,  0.3140, -0.0025],\n",
      "        [-0.2345, -0.1029, -0.1584,  ..., -0.0821, -0.0126,  0.1765],\n",
      "        [-0.1633, -0.2002, -0.0410,  ...,  0.1477,  0.0554,  0.0626],\n",
      "        ...,\n",
      "        [ 0.2236, -0.1623,  1.3227,  ..., -0.3954, -0.3976, -0.2058],\n",
      "        [ 0.2130,  0.0487, -0.0797,  ..., -0.0021,  0.1804, -0.1791],\n",
      "        [-0.3188, -0.1977,  0.0807,  ...,  0.1682,  0.3072,  0.4516]],\n",
      "       device='cuda:0', dtype=torch.float64)), ('fc1.bias', tensor([-0.2820, -0.1360,  0.0989,  0.1889, -0.0641, -0.1444,  0.0774, -0.0854,\n",
      "        -0.0723, -0.4619, -0.3242, -0.1792, -0.3837,  0.2216, -0.1814,  0.1091,\n",
      "         0.1473, -0.2031,  0.0197, -0.2613,  0.2230, -0.2787,  0.1623, -0.3064,\n",
      "         0.1222,  0.0346,  0.0738,  0.0609,  0.0628,  0.0742, -0.3362, -0.0412,\n",
      "         0.0811, -0.1205, -0.4240,  0.1431,  0.1932, -0.2972, -0.0275, -0.1051,\n",
      "        -0.0322, -0.1811, -0.0120,  0.1217,  0.4043,  0.1716, -0.0404, -0.1997,\n",
      "         0.2036, -0.2191, -0.1284,  0.1523,  0.1631, -0.0247,  0.0826, -0.0656,\n",
      "        -0.4968,  0.0140, -0.1768,  0.0252,  0.0229,  0.2686, -0.2357, -0.0689,\n",
      "         0.0249,  0.1135, -0.4197,  0.0217,  0.1478, -0.0444,  0.0564, -0.1428,\n",
      "         0.0793, -0.1142, -0.2746,  0.0966, -0.4287,  0.0639, -0.1728,  0.2409,\n",
      "        -0.0119,  0.3663, -0.0527, -0.1769,  0.0636,  0.0705,  0.0140, -0.2291,\n",
      "        -0.4209,  0.0542, -0.2091, -0.2855,  0.0469, -0.0516, -0.0307,  0.0307,\n",
      "         0.2258, -0.2112,  0.0904, -0.1822, -0.1990,  0.1042,  0.0413, -0.3946,\n",
      "        -0.0240, -0.0530,  0.2732, -0.4165,  0.0988,  0.0337,  0.1733, -0.3363,\n",
      "         0.0417, -0.2379,  0.1413, -0.5983, -0.0020, -0.1064, -0.0081, -0.0338,\n",
      "         0.0397,  0.0416,  0.1179, -0.3685,  0.0416,  0.5554,  0.0071, -0.1631],\n",
      "       device='cuda:0', dtype=torch.float64)), ('fc2.weight', tensor([[ 0.0336, -0.0141,  0.2543,  ...,  0.1402, -0.0167,  0.2421],\n",
      "        [-0.2245, -0.1307, -0.3071,  ..., -0.2254, -0.0793, -0.0649],\n",
      "        [-0.0021, -0.1043, -0.2323,  ...,  0.0464,  0.0717, -0.0336],\n",
      "        ...,\n",
      "        [-0.3590,  0.1034, -0.3518,  ..., -0.3986, -0.0488, -0.1913],\n",
      "        [-0.0483, -0.0473,  0.1761,  ...,  0.0222, -0.0178,  0.2189],\n",
      "        [-0.0059, -0.0582,  0.2495,  ...,  0.0773, -0.0428,  0.1952]],\n",
      "       device='cuda:0', dtype=torch.float64)), ('fc2.bias', tensor([ 0.8240, -0.0267,  1.9293, -0.1259, -0.0470, -0.2149, -0.0706, -0.1899,\n",
      "        -0.0525,  1.3434,  0.8637, -0.1408,  0.9182,  1.0207,  1.1757,  1.0857,\n",
      "        -0.2865,  0.0581, -0.0737, -0.3725, -0.3323,  1.3661,  2.6240, -0.4527,\n",
      "        -0.0585,  1.7332, -0.0823, -0.3791,  0.0651,  1.1794,  1.9071,  0.1016,\n",
      "         1.3109, -0.2924,  1.8309, -0.5705,  1.9264, -0.1287,  0.0341,  0.0390,\n",
      "        -0.1163,  1.1606, -0.1155, -0.0190, -0.5115, -0.0196, -0.2479,  1.9185,\n",
      "         1.9584,  1.4135, -0.3618, -0.0771,  2.0495, -0.0202,  1.9232,  0.0720,\n",
      "        -0.5273, -0.0968,  0.1259,  1.0498, -0.0486,  0.0141,  1.6483,  1.8795,\n",
      "         1.2566,  0.1983, -0.0184,  1.8439, -0.1010,  1.4446,  1.4022,  1.9114,\n",
      "         0.0576, -0.0098,  0.0274,  0.0567, -0.1111, -0.1579,  0.0292,  0.9076,\n",
      "        -0.4543, -0.4086,  1.1917, -0.4788,  1.2918,  0.5100, -0.0584, -0.2136,\n",
      "        -0.5135, -0.0712,  1.8092, -0.2654,  0.7511,  0.3304,  1.8135,  1.5542,\n",
      "         0.0570,  1.8666,  1.9385,  0.0280, -0.2015,  0.0608, -0.3184,  0.7496,\n",
      "         1.7646,  1.8534,  1.8300,  1.6935,  1.9859, -0.0403,  1.7372,  1.6743,\n",
      "         1.3970,  0.7529, -0.0360, -0.0921, -0.4758, -0.0241, -0.0796, -0.4789,\n",
      "        -0.0053,  0.0378,  0.1426,  1.8489,  1.4524,  0.0052, -0.3956, -0.1465],\n",
      "       device='cuda:0', dtype=torch.float64)), ('fc3.weight', tensor([[-0.1212, -0.1532,  0.0532,  ..., -0.1556, -0.2564, -0.1470],\n",
      "        [ 0.1366,  0.0239, -0.1244,  ...,  0.0840, -0.1975, -0.0053],\n",
      "        [ 0.3583, -0.1372,  0.1952,  ..., -0.2867,  0.3149,  0.3508],\n",
      "        ...,\n",
      "        [-0.1545, -0.2065,  0.0150,  ..., -0.1376, -0.1138, -0.1493],\n",
      "        [ 0.3303, -0.0387,  0.2516,  ..., -0.2432,  0.3080,  0.3783],\n",
      "        [ 0.4173, -0.0158,  0.2507,  ..., -0.1751,  0.3430,  0.2989]],\n",
      "       device='cuda:0', dtype=torch.float64)), ('fc3.bias', tensor([-0.1830, -0.0596,  0.2470,  0.0236, -0.1603,  0.0419, -0.1476, -0.0131,\n",
      "        -0.0538, -0.0338,  0.0130, -0.1917,  1.1031,  0.0160,  0.0791, -0.0128,\n",
      "        -0.1147,  0.1264, -0.1830, -0.2206, -0.0514, -0.1514, -0.0712,  0.1872,\n",
      "         0.0251, -0.0608, -0.0697, -0.1901, -0.0403, -0.0294, -0.1002, -0.0313,\n",
      "         0.0171, -0.1579,  0.2346, -0.0763, -0.2794, -0.0993,  0.1759, -0.0563,\n",
      "         0.2651, -0.0228, -0.0179,  1.2876, -0.0770, -0.1948, -0.0562,  0.1497,\n",
      "        -0.1044, -0.2236, -0.1592, -0.0694, -0.0388, -0.2135, -0.0475, -0.0578,\n",
      "        -0.1745, -0.0338, -0.0536,  0.2200,  0.0377,  0.0511, -0.1595, -0.0927,\n",
      "         0.2281, -0.0038, -0.0938, -0.0099, -0.0094,  0.0104, -0.0062,  0.2193,\n",
      "        -0.2339, -0.1824, -0.0821,  0.0502, -0.2915,  0.2139, -0.0662,  0.1491,\n",
      "        -0.2444,  0.0640, -0.2156, -0.1384, -0.0562, -0.1162, -0.2051,  0.1873,\n",
      "         0.3422, -0.1317,  0.1974, -0.1407,  0.1941, -0.1293, -0.0675, -0.2020,\n",
      "         0.0505,  0.0996, -0.2971,  0.0223,  0.0136,  0.0214,  0.2324, -0.1531,\n",
      "        -0.0786, -0.0178,  0.0721, -0.1343, -0.1564, -0.2457,  0.5002,  0.0812,\n",
      "        -0.2525, -0.1931, -0.2065, -0.1619, -0.0109, -0.1879, -0.1162, -0.0803,\n",
      "        -0.0627, -0.0047,  0.3645,  0.0458, -0.1663, -0.1455,  0.0411,  0.3971],\n",
      "       device='cuda:0', dtype=torch.float64)), ('fc4.weight', tensor([[-0.0229,  0.0340,  0.6551,  ...,  0.1089,  0.5250,  0.5938],\n",
      "        [-0.1470,  0.1312,  0.3482,  ..., -0.0988,  0.2721,  0.2795],\n",
      "        [-0.1200, -0.2863,  0.2429,  ..., -0.0956,  0.0969,  0.1736],\n",
      "        ...,\n",
      "        [-0.2348, -0.3966,  0.2151,  ..., -0.1291,  0.2578,  0.1410],\n",
      "        [ 0.1496, -0.1942, -0.2978,  ...,  0.1020, -0.2785, -0.2976],\n",
      "        [ 0.1490, -0.0886, -0.1816,  ...,  0.1399, -0.2986, -0.2492]],\n",
      "       device='cuda:0', dtype=torch.float64)), ('fc4.bias', tensor([ 2.8490e-01,  1.2934e-01,  3.4354e-02,  4.3892e-02, -3.7560e-02,\n",
      "        -1.1531e-01, -4.7528e-02,  2.0732e-01, -2.6645e-01, -1.5313e-01,\n",
      "        -1.2974e-01, -1.5763e-01, -8.5187e-02,  5.5590e-03, -1.3221e-01,\n",
      "        -7.1797e-02, -6.9279e-02,  1.6449e-01,  8.5628e-02, -9.0550e-02,\n",
      "         1.8416e-01, -1.7524e-01, -1.8046e-01,  1.2868e-01, -6.3322e-02,\n",
      "         1.2109e-01, -5.1355e-02,  2.5210e-01,  4.8833e-01, -7.9128e-02,\n",
      "         5.2926e-01, -5.8830e-02, -6.5646e-02,  1.6855e-01,  1.4070e-01,\n",
      "        -1.3651e-01, -4.9224e-02,  1.1702e-01,  9.9948e-02, -5.4250e-02,\n",
      "        -1.8173e-01, -4.5919e-02,  1.6505e-01, -6.7822e-02, -5.4687e-02,\n",
      "         4.9526e-01,  2.3907e-01,  2.4997e-01,  4.5198e-03, -5.6211e-02,\n",
      "        -2.9234e-02, -1.5797e-01, -8.2764e-02, -4.5985e-02, -6.2517e-02,\n",
      "         1.4549e-01, -9.3250e-02,  3.9060e-02,  8.2645e-02,  1.5003e-01,\n",
      "        -6.9021e-02, -4.7957e-02, -1.0380e-01,  1.4851e-01, -9.3482e-02,\n",
      "        -2.1259e-01, -4.7492e-02, -6.9746e-02, -6.8423e-02,  5.1951e-02,\n",
      "         4.4731e-01, -1.3424e-01, -1.5055e-01, -1.3940e-01, -9.4753e-02,\n",
      "         1.5121e-01, -1.1014e-01, -5.5090e-02, -7.7542e-02,  2.0636e-01,\n",
      "        -1.5605e-01,  2.6984e-01, -3.8611e-02,  1.9771e-03, -1.1968e-01,\n",
      "         1.5912e-01, -9.3778e-02, -1.8049e-01, -1.8571e-01, -6.6736e-02,\n",
      "        -1.2712e-01, -1.7323e-02,  4.6956e-01, -1.8883e-01, -9.2813e-02,\n",
      "        -1.6670e-01,  3.9512e-01,  8.0119e-02, -1.0996e-01, -2.1205e-03,\n",
      "         1.2429e-01, -2.2403e-01,  1.4548e-01, -1.5543e-01,  3.0998e-01,\n",
      "        -1.2485e-01,  1.0295e-01, -1.0556e-02, -1.3733e-01,  5.4658e-04,\n",
      "        -3.9194e-02,  3.6607e-02,  4.8522e-01,  4.1721e-01, -4.3159e-02,\n",
      "         6.2083e-01,  1.6825e-01,  5.5681e-02,  1.0250e-01, -1.7877e-02,\n",
      "         1.4798e-01, -1.0165e-01, -1.2949e-01,  3.8754e-01, -1.0121e-01,\n",
      "         1.0288e-01, -1.9455e-01, -8.5861e-02], device='cuda:0',\n",
      "       dtype=torch.float64)), ('fc5.weight', tensor([[-2.0557, -0.4779, -1.2911, -1.0406, -0.4510,  0.0209, -0.1764, -0.7098,\n",
      "          0.0273,  0.0132, -0.3901,  0.1005,  0.0236, -0.3147,  0.0445, -0.0119,\n",
      "          0.0563, -1.3153, -1.1952,  0.0846, -0.7178,  0.0734,  0.0564, -1.1938,\n",
      "         -0.0193, -1.0529, -0.4475, -0.6183, -3.0178, -0.5956, -3.0643, -0.4704,\n",
      "          0.0266, -1.0649, -1.3661,  0.0813, -0.3774, -0.6817, -1.3139,  0.1300,\n",
      "          0.1796, -0.3764, -1.8823,  0.1152,  0.0929, -2.2705, -0.7394, -0.6545,\n",
      "         -0.3291,  0.0389,  1.4920, -0.6145, -0.5285, -0.0614, -0.7029, -1.1593,\n",
      "          0.1080, -0.0097, -1.1648, -1.1824, -0.4405, -0.4701,  0.0549, -1.8611,\n",
      "          0.0369,  0.0484,  0.0640, -0.0260, -0.4403, -1.1559, -3.0625, -0.7913,\n",
      "          0.1106,  0.1114,  0.0328, -1.1100,  0.0809,  0.0802,  0.0152, -1.7895,\n",
      "          0.0939, -0.7721, -0.0555, -0.4786,  0.1520, -1.2163, -0.0127,  0.1052,\n",
      "          0.0971, -0.3953,  0.0456, -0.0412, -3.0323,  0.0531,  0.1262,  0.0918,\n",
      "         -2.0579, -1.5620, -0.0095, -0.1596, -1.2916, -0.0060, -0.9042,  0.0204,\n",
      "         -0.7253,  0.1897, -1.3479, -0.0285,  0.0849, -0.0155,  0.0733, -1.3090,\n",
      "         -2.9775, -1.9400,  0.1347, -2.6043, -1.9166, -0.4998, -1.6180, -0.4658,\n",
      "         -1.1165, -0.4152,  0.1171, -1.6544,  0.0973, -1.4672,  0.0673,  0.1211],\n",
      "        [-1.9221, -0.3193, -1.0360, -0.9789, -0.2295,  0.4179,  0.0834, -0.5723,\n",
      "          0.6098,  0.3766, -0.2959,  0.6020,  0.5570, -0.2531,  0.6264, -0.2197,\n",
      "          0.5055, -0.9969, -1.1078,  0.6161, -0.6285,  0.6070,  0.4784, -0.9519,\n",
      "         -0.0106, -0.9709, -0.2587, -0.5685, -3.2170, -0.5208, -2.8419, -0.2363,\n",
      "          0.5489, -0.8396, -1.0421,  0.5330, -0.2121, -0.4947, -0.9350,  0.5973,\n",
      "          0.6234, -0.2812, -1.7935,  0.5637,  0.5294, -2.3472, -0.6410, -0.4783,\n",
      "         -0.2683,  0.6013,  1.0863, -0.3603, -0.4361,  0.0550, -0.4360, -1.1593,\n",
      "          0.5707,  0.0326, -1.2040, -0.9421, -0.3158, -0.3589,  0.5457, -1.6979,\n",
      "          0.6277,  0.4924,  0.0488,  0.0626, -0.2674, -1.0180, -3.1591, -0.5150,\n",
      "          0.5514,  0.4421,  0.5665, -0.9927,  0.4906,  0.5126,  0.5569, -1.4665,\n",
      "          0.5925, -0.6392, -0.0802, -0.2770,  0.6493, -0.9796, -0.0849,  0.5966,\n",
      "          0.5456, -0.1791,  0.4716,  0.0363, -2.9874,  0.4504,  0.5999,  0.5425,\n",
      "         -1.9126, -1.3005, -0.0932,  0.0446, -0.8829,  0.5885, -1.2728,  0.5236,\n",
      "         -0.5751,  0.5683, -0.9462,  0.0511,  0.5323, -0.0231,  0.6117, -1.3090,\n",
      "         -2.8962, -2.5619,  0.5870, -2.5761, -1.6822, -0.3032, -1.4364, -0.2082,\n",
      "         -1.0970, -0.1626,  0.4846, -1.6751,  0.5150, -1.3299,  0.4175,  0.5873]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       device='cuda:0', dtype=torch.float64)), ('fc5.bias', tensor([-0.2751,  0.1751], device='cuda:0', dtype=torch.float64))])\n"
     ]
    }
   ],
   "source": [
    "env = Environment(map, adj, car_limit, ep, step)\n",
    "env.agent.brain.get_param(\"params/param\")\n",
    "\n",
    "env.run()\n",
    "\n",
    "print(env.agent.brain.model.state_dict())\n",
    "env.agent.brain.save_param(\"params/param\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12479996222092404"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.uniform(0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OrderedDict([('fc1.weight', tensor([[ 0.1824, -0.0464,  0.0901,  ..., -0.0926, -0.1155, -0.4320],\n",
    "        [-0.1883, -0.0564, -0.0636,  ..., -0.1283, -0.0588,  0.1303],\n",
    "        [ 0.0165, -0.2002, -0.1623,  ..., -0.0321, -0.1244, -0.1171],\n",
    "        ...,\n",
    "        [ 0.0292, -0.1860,  0.2719,  ..., -0.2010, -0.2032, -0.0114],\n",
    "        [ 0.2130,  0.0487, -0.0798,  ..., -0.0021,  0.1804, -0.1791],\n",
    "        [-0.3188, -0.1977,  0.0807,  ...,  0.1682,  0.3072,  0.4516]],\n",
    "       device='cuda:0', dtype=torch.float64)), ('fc1.bias', tensor([ 0.1475, -0.0898,  0.2787,  0.2013,  0.1294, -0.1444,  0.3007, -0.0523,\n",
    "        -0.0723, -0.3733, -0.2445, -0.1790, -0.3836, -0.0865, -0.1814, -0.1508,\n",
    "        -0.1147, -0.2031,  0.0707, -0.2613,  0.2230, -0.1986,  0.1623, -0.3064,\n",
    "         0.0739,  0.2436, -0.1281,  0.0609,  0.0628,  0.0742, -0.3352, -0.0412,\n",
    "         0.0811, -0.3991, -0.3922, -0.1117,  0.1932, -0.2706, -0.0274, -0.1051,\n",
    "        -0.0322, -0.1521, -0.0120,  0.1544,  0.4070,  0.1716, -0.0404, -0.1112,\n",
    "         0.2036, -0.2191, -0.1266,  0.1523,  0.2324, -0.0240,  0.0838, -0.0656,\n",
    "        -0.3309,  0.2157, -0.1270,  0.0469,  0.2272,  0.3494, -0.1997, -0.0689,\n",
    "         0.0253,  0.1479, -0.4197,  0.0217,  0.1846,  0.4082,  0.0564,  0.1996,\n",
    "         0.4547, -0.1142, -0.2740,  0.1022, -0.3534,  0.0639, -0.1181,  0.2650,\n",
    "        -0.0006,  0.3717,  0.0025, -0.1769,  0.0636,  0.0705,  0.0198, -0.0079,\n",
    "        -0.4209,  0.0542, -0.2091,  0.1331,  0.0469, -0.0516, -0.0307,  0.0640,\n",
    "         0.2258, -0.1891,  0.0903, -0.1822, -0.1990,  0.1042,  0.0480, -0.3946,\n",
    "        -0.0240, -0.3179,  0.2764, -0.4160,  0.0988,  0.1386,  0.1733, -0.3363,\n",
    "         0.0417, -0.2177,  0.2247, -0.3794, -0.2737, -0.1065, -0.0081, -0.0338,\n",
    "         0.0397,  0.0416,  0.1179, -0.3685, -0.1984,  0.3610,  0.0071, -0.1631],\n",
    "       device='cuda:0', dtype=torch.float64)), ('fc2.weight', tensor([[ 0.2410,  0.0442,  0.4154,  ...,  0.3079, -0.0167,  0.2421],\n",
    "        [-0.2245, -0.0342, -0.3070,  ..., -0.2254, -0.0793, -0.0649],\n",
    "        [ 0.0378, -0.0817, -0.0151,  ..., -0.0205,  0.0717, -0.0336],\n",
    "        ...,\n",
    "        [-0.3589,  0.0375, -0.3516,  ..., -0.3986, -0.0488, -0.1913],\n",
    "        [ 0.1914, -0.0339,  0.1843,  ...,  0.2591, -0.0178,  0.2189],\n",
    "        [ 0.2374, -0.0525,  0.3668,  ...,  0.3216, -0.0428,  0.1952]],\n",
    "       device='cuda:0', dtype=torch.float64)), ('fc2.bias', tensor([ 2.8216e-01, -2.0771e-01,  3.5479e-02, -3.8576e-02, -1.4380e-01,\n",
    "         2.9497e-02, -4.8456e-02, -2.8399e-01, -2.2882e-01,  2.7046e-01,\n",
    "         2.9535e-01,  4.7131e-02,  2.7400e-01, -7.6377e-02, -1.5518e-02,\n",
    "        -4.2317e-02, -2.3332e-01,  2.1120e-01, -1.8313e-01,  1.8472e-01,\n",
    "         2.3316e-01,  2.8204e-01, -6.7554e-03,  2.1732e-01, -2.2266e-01,\n",
    "         1.5507e-02, -8.0301e-05,  1.8835e-01, -3.0531e-02, -3.4607e-02,\n",
    "         1.5089e-02, -3.9384e-02, -3.7193e-02,  2.3002e-01,  6.1446e-02,\n",
    "         6.7815e-02,  5.5900e-02, -2.5312e-03, -1.9331e-01,  1.6608e-02,\n",
    "        -2.4413e-01,  3.1744e-01, -1.1589e-01,  1.3784e-01,  5.0960e-02,\n",
    "        -2.7876e-02, -4.4386e-02,  5.6833e-02,  5.7828e-02,  2.7353e-01,\n",
    "         2.2045e-01, -2.3096e-01,  7.7636e-02, -4.4815e-02,  6.2795e-02,\n",
    "        -1.6973e-01,  1.5255e-01, -2.4897e-01, -4.6276e-02, -4.4034e-02,\n",
    "        -1.4108e-01, -2.5305e-02,  3.2598e-02,  5.8388e-02,  2.6805e-01,\n",
    "        -7.4055e-02, -1.4982e-01,  7.0535e-02, -2.9547e-01,  3.0805e-01,\n",
    "         3.1075e-01,  4.6418e-02, -1.3604e-01, -1.9921e-01, -1.4364e-01,\n",
    "        -7.4680e-02, -2.0963e-01,  2.4471e-02, -1.6036e-01, -7.2160e-02,\n",
    "         1.6027e-01,  2.1496e-01, -3.4007e-02,  9.5958e-02,  2.8028e-01,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
